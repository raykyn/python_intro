{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling in Python\n",
    "In diesem Notebook wird in Kürze das Topic Modeling anhand der Sturm-Briefedition vorgeführt.\n",
    "\n",
    "In einem ersten Schritt extrahieren wir den Text ohne Annotationen aus den XML-Dokumenten, in einem zweiten Schritt führen wir das Topic Modeling durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import der relevanten Bibliotheken\n",
    "from lxml import etree as et\n",
    "import glob\n",
    "import re\n",
    "\n",
    "texts = []\n",
    "\n",
    "for letter in glob.glob(\"docs/*.xml\"):\n",
    "    tree = et.parse(letter)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # den Haupttext greifen\n",
    "    paragraphs = root.xpath(\"./tei:text/tei:body/tei:div[@type='content']/tei:p\", namespaces={\"tei\":\"http://www.tei-c.org/ns/1.0\"})\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # den Text von Tags und Kommentaren bereinigen\n",
    "        et.strip_elements(paragraph, \"{http://www.tei-c.org/ns/1.0}note\", with_tail=False)\n",
    "        paragraph_text = et.tostring(paragraph, method=\"text\", encoding=\"utf8\", with_tail=False).decode(\"utf8\")\n",
    "        texts.append(paragraph_text)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Texte möchten wir nun noch vorverarbeiten. Dazu gehört einerseits das *tokenisieren* also auflösen in Worteinheiten, das Entfernen von Stoppwörtern und die Lemmatisierung von Wörtern, also die Reduktion auf eine Grundform (z.B. Infinitiv).\n",
    "\n",
    "Für das Preprocessing empfiehlt sich bei grösseren Projekten eine NLP-Bibliothek wie [spacy](https://spacy.io/). Spacy ist eine NLP-Bibliothek, die eine komplette Pipeline, u.a. auch Named-Entity- und POS-Tagging unterstützt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U spacy\n",
    "\n",
    "# und das deutsche Modell\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für Spacy müssen wir ein bestimmtes Modell herunterladen, mit dem wir die Texte vorbereiten wollen.\n",
    "# Wir nehmen für die Sturm-Edition das kleine deutsche Modell, das ist auch schnell heruntergeladen\n",
    "# Wir schalten aber die Module ab, die wir nicht benötigen, damit die Texte schneller bearbeitet werden können.\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    # wir geben das Lemma zurück falls es kein Stopwort ist und alphanumerisch ist\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "# Listenkomprehension ersetzt hier eine for-Schleife, tut aber dasselbe\n",
    "prepped_texts = [preprocess(text) for text in texts]\n",
    "\n",
    "print(prepped_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das Topic Modeling verwenden wir hier [Tomotopy](https://bab2min.github.io/tomotopy/v0.13.0/en/), ein Package, das in Python sehr einfach anwendbar ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "# Initialisiere das Modell, k gibt die Zahl der gewünschten Topics an\n",
    "mdl = tp.LDAModel(k=5)\n",
    "\n",
    "# Lade alle Texte ins Modell\n",
    "for text in prepped_texts:\n",
    "    mdl.add_doc(text)\n",
    "\n",
    "# Trainiere das Modell, alle 100 Trainings-Iterationen wird der Stand ausgegeben\n",
    "for i in range(0, 1000, 100):\n",
    "    mdl.train(100)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "\n",
    "# Zeige, welche Topics mit welchen wichtigen Wörtern erkannt wurden\n",
    "for k in range(mdl.k):\n",
    "    print('Top 10 words of topic #{}'.format(k))\n",
    "    print(mdl.get_topic_words(k, top_n=10))\n",
    "\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Viewer können wir unser Modell inspizieren. Mit der geringen Menge an Dokumenten in unserem Fall ist es nicht überraschend, dass die Topics eher ein durcheinander sind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.viewer.open_viewer(mdl, port=9999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HistArbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
